---
title: Thesis Proposal Review
description: TPR Slides August 2017
layout: slide
date: 2017-08-30
reveal_config:
  controls: true
  progress: false
  transition: slide
---

<section>

  <h2>Computing Culture; a humanities approach to artificial neural networks</h2>

  <h3>
    Kieran Browne &mdash; Thesis Proposal Review
  </h3>

  <aside class="notes">
    <ul>
      <li>Critical approaches to data</li>
      <li></li>
    </ul>
  </aside>
</section>

<section>
  <h2>Hype about deep neural networks</h2>
  <ul>
    <li>The hottest thing in AI</li>
  </ul>

  <aside class="notes">
    <ul>
      <li>Neural networks are a class of machine learning algorithm inspired by the brain</li>
      <li>They are the algorithm behind</li>
      <li>Self-driving cars</li>
      <li>Networks that compose poetry</li>
    </ul>
  </aside>
</section>

<section class="black tall">
  <h2>What's a neural network?</h2>
  <img class="full" src="/images/neuron.jpg">

  <aside class="notes">
    <ul>
      <li>A neuron is the cell you have in your brain</li>
      <li>There's about 86 billion in the human brain</li>
      <li><i>Explain</i></li>
    </ul>
  </aside>
</section>

<section class="white">
  <h2>What's a neural network?</h2>
  <img class="full" src="/images/artificial-neural-net-diagram.jpg">

  <aside class="notes">
    <ul>
      <li>We think of computers as being strictly logical</li>
      <li><i>if this then that</i></li>
      <li>Intelligence is not encoded in symbolic transformations</li>
      <li>It's emergent intelligence</li>
      <li>Each neuron is not intelligent but the whole is</li>
      <li>It feels like this shouldn't work</li>
    </ul>
  </aside>
</section>

<section class="">
  <h2>input <br>&darr;<br> [Black Box] <br>&darr;<br> output</h2>

  <aside class="notes">
    <ul>
      <li>A black box refers to a system whose internal patterns and process are inscrutable</li>
    </ul>
  </aside>
</section>

<section class="">
  <h2><img src="http://placebear.com/200/200"><br>&darr;<br> [ ??? ] <br>&darr;<br> "It's a bear"</h2>

  <aside class="notes">
    <ul>
      <li>Neural networks are black boxes</li>
      <li>BB problem cited many times as a barrier to adoption</li>
      <li>But after some major successes in image recognition tasks people have started using them anyway</li>
      <li>Levels of abstraction</li>
    </ul>
  </aside>
</section>

<section class="">

  <h2>Neural networks infer their own semantic meaning</h2>


  <aside class="notes">
    <ul>
      <li></li>
    </ul>
  </aside>
</section>


<section class="">

  <h2>The nightmare scenario</h2>

  <img src="https://web.archive.org/web/20170810051446im_/https://pbs.twimg.com/media/C9sx1jLWAAAkhtK.jpg">

  <aside class="notes">
    <ul>
      <li>Image from faceapp</li>
      <li>FaceApp can do some impressive stuff</li>
      <li>But found that their "Hot" filter gave users white skin.</li>
      <li>How the heck did this happen?</li>
      <li>Its possible to post-rationalise about the dataset from the discovered pattern</li>
    </ul>
  </aside>
</section>


<section class="">

  <h2>The way we use and understand neural networks is built on realist models of knowledge</h2>

  <aside class="notes">
    <ul>
      <li>These are encultured systems.</li>
      <li>There is culture latent in human data</li>
      <li>Neural networks will extract any salient patterns in data</li>
      <li>Including cultural patterns</li>
    </ul>
  </aside>
</section>

<section>
  <h2>Project Scope</h2>
</section>

<section>
  <h2>Theory</h2>
  <h3>Johanna Drucker</h3>

  <aside class="notes">
    <ul>
      <li>Data as capta</li>
      <li>Constructedness of data</li>
      <li>"intellectual Trojan horse"</li>
      <li>Drucker describes the creeping of realist modes of knowledge into the humanities through graphical display</li>
      <li>I see my project doing the opposite, cultural ways of knowing have made their way into computer science</li>
      <li>Neural networks will reify the culture latent in the training data.</li>
      <li>The goal is to lend ways of knowing from the humanities to computer science</li>
    </ul>
  </aside>
</section>

<section>
  <h3>Humanities Approaches to Graphical Display</h3>
  <p>
  This is the hallmark of realist models of knowledge and needs to be 
  subjected to a radical critique to return the humanistic tenets of 
  constructedness and interpretation to the fore. Realist approaches 
  depend above all upon an idea that phenomena are observer-independent 
  and can be characterized as data.
  </p>

  <aside class="notes">
    <ul>
      <li>Data as capta</li>
      <li>Constructedness of data</li>
      <li>"intellectual Trojan horse"</li>
    </ul>
  </aside>
</section>

<section>
  <h2>Related Work</h2>
  <ul>
    <li>Behavioural visualisations</li>
  </ul>
</section>

<section>
  <h2>Methodologies</h2>
  <h3>Practice-led research</h3>
  <h3>Criticism</h3>
</section>

<section>
  <h2>Proposed directions</h2>
  <ul>
    <li>Exploratory interfaces</li>
    <li>Collective sense making</li>
    <li>Behavioural visualisations</li>
  </ul>
</section>


<div style="display: none; font-size: 0;">
Artificial neural networks are a potent technology capable of extracting 
and reproducing complex patterns from data, including  cultural,
linguistic and visual processes previously beyond the reach of 
computation. However, the internal functions of neural networks are 
notoriously difficult to understand. The researchers and technicians 
who design and train neural networks can rarely explain their behaviour. 
This is particularly problematic in cases where neural networks learn, 
and thus perpetuate cultural biases. My research inherits its 
theoretical framework from the (digital) humanities, particularly the 
ideas of Johanna Drucker regarding graphical display and constructionist 
notions of data. I intend to make use of mixed methods in my research. 
This includes critical analysis of existing representations of neural 
networks and practice-led research, specifically developing experimental 
visualisations and novel exploratory interfaces.


- introduction
- what's a neural network
- what's a black box
  - why is the neural network a black box
  - the black box getting darker
  - semantics
    - we want our explanations in terms of meaningful concepts
    - extracting semantics
      - problems extracting semantics
      - marked and unmarked categories

      - networks often uncritically applied to cultural expressive forms

- why is this a problem
  - examples of cultural bias in neural networks
  - computer science is illequipped handle questions of culture and bias

- code needs criticism

- scope
  - 

- theory
  - Johanna Drucker
  - humanities approaches to (graphical display/ interface theory)
  - data vs capta

- precedents (why visualisation)
  - knowledge as visually mediated
  - when you have a hammer everything looks like a nail
    - attempts to visualise neural networks with techniques for data vis and graphical user interfaces; but neural networks are not data and not much like normal digital tech

- methodologies
  - practice led research
    - research through making
      - interfaces
      - visualisations
      - critical artworks

  - critical approach to neural networks
    - critical approach to data (CAPTA)
    - redescribing a neural networks as cultral computing
    - neural networks as cultural actors
    - training as enculturing
    - what are the semantics of internal representations

- originality
  - related work

</div>
