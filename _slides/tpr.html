---
title: Thesis Proposal Review
description: TPR Slides August 2017
layout: slide
date: 2017-08-30
reveal_config:
  controls: true
  progress: false
  transition: slide
---

<section>

  <h2>Computing Culture; a humanities approach to artificial neural networks</h2>

  <h3>
    Kieran Browne - Thesis Proposal Review
  </h3>

  <aside class="notes">
    Oh hey, these are some notes. 
    - They'll be hidden in your presentation, but you can see them if you open the speaker notes window (hit 's' on your keyboard).
  </aside>
</section>

<section class="black tall">
  <h2>What's a neural network?</h2>
  <img class="full" src="/images/neuron.jpg">

  <aside class="notes">
    Oh hey, these are some notes. 
    - They'll be hidden in your presentation, but you can see them if you open the speaker notes window (hit 's' on your keyboard).
  </aside>
</section>

<section class="white">
  <h2>What's a neural network?</h2>
  <img class="full" src="/images/artificial-neural-net-diagram.jpg">
</section>

<section>
  <blockquote>
    If there were machines which bore a resemblance to our bodies and imitated our actions as closely as possible for all practical purposes, we should still have two very certain means of recognizing that they were not real men. The first is that they could never use words, or put together signs, as we do in order to declare our thoughts to others.
  </blockquote>  
  <blockquote>
    <i>Discourse on the Method</i> &mdash; Ren√© Descartes
  </blockquote>
</section>

Artificial neural networks are a potent technology capable of extracting and reproducing complex patterns from data, including  cultural, linguistic and visual processes previously beyond the reach of computation. However, the internal functions of neural networks are notoriously difficult to understand. The researchers and technicians who design and train neural networks can rarely explain their behaviour. This is particularly problematic in cases where neural networks learn, and thus perpetuate cultural biases. My research inherits its theoretical framework from the (digital) humanities, particularly the ideas of Johanna Drucker regarding graphical display and constructionist notions of data. I intend to make use of mixed methods in my research. This includes critical analysis of existing representations of neural networks and practice-led research, specifically developing experimental visualisations and novel exploratory interfaces.


- introduction
- what's a neural network
- what's a black box
  - why is the neural network a black box
  - the black box getting darker
  - semantics
    - we want our explanations in terms of meaningful concepts
    - extracting semantics
      - problems extracting semantics
      - marked and unmarked categories

- why is this a problem
  - examples of cultural bias in neural networks
  - computer science is illequipped handle questions of culture and bias

- code needs criticism

- scope
  - 

- theory
  - Johanna Drucker
  - humanities approaches to (graphical display/ interface theory)
  - data vs capta

- precedents (why visualisation)
  - knowledge as visually mediated
  - when you have a hammer everything looks like a nail
    - attempts to visualise neural networks with techniques for data vis and graphical user interfaces; but neural networks are not data and not much like normal digital tech

- methodologies
  - practice led research
    - research through making
      - interfaces
      - visualisations
      - critical artworks
  - critical approach to neural networks
    - critical approach to data (CAPTA)
    - redescribing a neural networks as cultral computing
    - networks as cultural actors
    - training as enculturing
    - what are the semantics of internal representations

- originality
  - related work
