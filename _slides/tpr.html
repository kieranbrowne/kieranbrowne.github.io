---
title: Thesis Proposal Review
description: TPR Slides August 2017
layout: slide
date: 2017-08-30
reveal_config:
  controls: true
  progress: false
  transition: slide
---

<section>

  <h2>Computing Culture; a humanities approach to artificial neural networks</h2>

  <h3>
    Kieran Browne &mdash; Thesis Proposal Review
  </h3>

  <aside class="notes">
    <ul>
      <li>Artificial neural networks are notoriously difficult to understand.</li>
      <li>Are a class of algorithm inspired by the brain</li>
      <li>They allow us to extract patterns from data and repeat these in future</li>

      <li>Critical approaches to data</li>
      <li>I'm going to argue that:</li>
      <li>Neural networks are a cultural way of knowing</li>
      <li>They are vulnerable to cultural biases</li>
      <li>They can enact learned biases on a grand scale</li>
      <li>They are the algorithm behind recent success in solving problems that were previously only possible by humans.</li>
      <li>They are universal approximators</li>
      <li>Most commonly, they are trained on an example dataset and mathematically approximate the patterns it contains.</li>
      <li>Their notable successes and generality has</li>
      <li>Drive cars</li>
      <li>Compose poetry</li>
      <li>Translate between languages</li>
      <li>Describe images</li>
      <li>Grant parole to inmates</li>
    </ul>
  </aside>
</section>

<section class="black tall">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/33PSr8jB8h4?rel=0" frameborder="0" allowfullscreen></iframe>

  <aside class="notes">
    <ul>
      <li>Neural networks are considered to be black boxes because it is difficult to understand its internal processes</li>
      <li>A black box refers to a system whose internal patterns and process are inscrutable</li>
      <li>We give them a bunch of data, and they transform it into some other form</li>
      <li>Training a network involves running a bunch of data through the network telling it where it went wrong and making small updates to improve its performance</li>
      <li>This process can approximate any pattern contained in data, even ones the researcher is not aware of</li>
      <li>This is a powerful technique because it gives us a handle on systems which are yet to be fully understood</li>
      <li>Neural networks are black boxes</li>
      <li>BB problem cited many times as a barrier to adoption</li>
      <li>But after some major successes in image recognition tasks people have started using them anyway</li>
      <li>Levels of abstraction</li>
      <li>It's not that we can't see inside neural networks as such, just that relationships between input and output are obfuscated in complexity.</li>
      <li>Nowadays we come into contact with neural networks all the time</li>
      <li>You might not care if google translate inexplicably gives you a bad translation every now and again</li>
      <li>You might feel differently about getting into a self-driving car, if no one knows how it works.</li>
      <li>We want our networks to explain themselves</li>
      <li>Semantic meaning is needed to make sense of these things</li>
      <li>For deep neural networks, the raw text or pixel data is rarely a we want our explanation for a self-driving car in terms of semi-trailers, pedestrians and cliffs</li>
      <li>We would like technicians to be able to explain the decisions of the network but this isn't always possible</li>
    </ul>
  </aside>
</section>

<section class="">
  <h2>The problem</h2>

  <aside class="notes">
    <ul>
      <li>ANNs occupy a curious ontological status because they conflate prediction with action</li>
      <li>They use the same mathematics that statisticians use to understand the world, to shape it</li>
      <li>Neural networks infer their own semantics</li>
    </ul>
  </aside>
</section>

<section class="">

  <h2>Semantics derived automatically from language corpora necessarily contain human biases</h2>
  <p>Caliskan, Aylin, Joanna J Bryson, and Arvind Narayanan. (2017).</p>

  <aside class="notes">
    <ul>
      <li></li>
    </ul>
  </aside>
</section>


<section class="">

  <h2>The nightmare scenario</h2>

  <img src="https://web.archive.org/web/20170810051446im_/https://pbs.twimg.com/media/C9sx1jLWAAAkhtK.jpg">

  <aside class="notes">
    <ul>
      <li>Image from faceapp</li>
      <li>FaceApp can do some impressive stuff</li>
      <li>But found that their "Hot" filter gave users white skin.</li>
      <li>How the heck did this happen?</li>
      <li>Its possible to post-rationalise about the dataset from the discovered pattern</li>
    </ul>
  </aside>
</section>

<section class="">

  <h2>Explaining visually</h2>

  <aside class="notes">
    <ul>
      <li>Why visualisation?</li>
      <li>Visualisation has had major successes in managing complexity</li>
      <li>More than ever before knowledge is visually mediated</li>
      <li>Our visual faculties have a number of properties which make them particularly good at expressing comparision</li>
    </ul>
  </aside>
</section>

<section class="">

  <h2>Explaining visually</h2>
  <img src="https://oregoneconomicanalysis.files.wordpress.com/2011/09/big5_emp_returntopeak1.jpg">

  <aside class="notes">
    <ul>
      <li>Why visualisation?</li>
      <li>Visualisation has had major successes in managing complexity</li>
      <li>More than ever before knowledge is visually mediated</li>
      <li>Our visual faculties have a number of properties which make them particularly good at expressing comparision</li>
    </ul>
  </aside>
</section>

<section class="">

  <h2>Explaining visually</h2>
  <img src="http://www.iphonefaq.org/files/styles/large/public/Fyuse.jpeg?itok=A37Yk_cq">

  <aside class="notes">
    <ul>
      <li>Why visualisation?</li>
      <li>Visualisation has had major successes in managing complexity</li>
      <li>More than ever before knowledge is visually mediated</li>
      <li>Our visual faculties have a number of properties which make them particularly good at expressing comparision</li>
    </ul>
  </aside>
</section>

<section>
  <h2>Theory</h2>
  <h3>Johanna Drucker</h3>

  <aside class="notes">
    <ul>
      <li>Data as capta</li>
      <li>Constructedness of data</li>
      <li>"intellectual Trojan horse"</li>
      <li>Drucker describes the creeping of realist modes of knowledge into the humanities through graphical display</li>
      <li>I see my project doing the opposite, cultural ways of knowing have made their way into computer science</li>
      <li>Neural networks will reify the culture latent in the training data.</li>
      <li>The goal is to lend ways of knowing from the humanities to computer science</li>
    </ul>
  </aside>
</section>

<section>
  <h3>Humanities Approaches to Graphical Display</h3>
  <p>Rethinking our approach to visualization and the assumptions that underpin it</p>
  <ul>
    <li>Constructivist notions of data</li>
    <li>Representing ambiguity</li>
    <li>Designing for interpretation and sense-making</li>
  </ul>

  <aside class="notes">
    <ul>
      <li>Data as capta</li>
      <li>Constructedness of data</li>
      <li>"intellectual Trojan horse"</li>
      <li>methods of presenting ambiguity and uncertainty in more nuanced terms would be useful</li>
    </ul>
  </aside>
</section>

<section class="">

  <h2>The way we use and understand neural networks is built on realist models of knowledge</h2>

  <aside class="notes">
    <ul>
      <li>These are encultured systems.</li>
      <li>There is culture latent in human data</li>
      <li>Neural networks will extract any salient patterns in data</li>
      <li>Including cultural patterns</li>
    </ul>
  </aside>
</section>

<section>
  <h2>Proposed methods</h2>
  <ul>
    <li>Critique current visual language of neural networks with care for the assumptions they bring.</li>
    <li>Design experimental visualisations and interfaces to explore the possibilities of visual representation</li>
  </ul>

  <aside class="notes">
    <ul>
      <li>Practice-led research</li>
      <li>Cultural criticism</li>
    </ul>
  </aside>
</section>

<section>
  <h2>Questions</h2>
</section>



<div style="display: none; font-size: 0;">
Artificial neural networks are a potent technology capable of extracting 
and reproducing complex patterns from data, including  cultural,
linguistic and visual processes previously beyond the reach of 
computation. However, the internal functions of neural networks are 
notoriously difficult to understand. The researchers and technicians 
who design and train neural networks can rarely explain their behaviour. 
This is particularly problematic in cases where neural networks learn, 
and thus perpetuate cultural biases. My research inherits its 
theoretical framework from the (digital) humanities, particularly the 
ideas of Johanna Drucker regarding graphical display and constructionist 
notions of data. I intend to make use of mixed methods in my research. 
This includes critical analysis of existing representations of neural 
networks and practice-led research, specifically developing experimental 
visualisations and novel exploratory interfaces.


- introduction
- what's a neural network
- what's a black box
  - why is the neural network a black box
  - the black box getting darker
  - semantics
    - we want our explanations in terms of meaningful concepts
    - extracting semantics
      - problems extracting semantics
      - marked and unmarked categories

      - networks often uncritically applied to cultural expressive forms

- why is this a problem
  - examples of cultural bias in neural networks
  - computer science is illequipped handle questions of culture and bias

- code needs criticism

- scope
  - 

- theory
  - Johanna Drucker
  - humanities approaches to (graphical display/ interface theory)
  - data vs capta

- precedents (why visualisation)
  - knowledge as visually mediated
  - when you have a hammer everything looks like a nail
    - attempts to visualise neural networks with techniques for data vis and graphical user interfaces; but neural networks are not data and not much like normal digital tech

- methodologies
  - practice led research
    - research through making
      - interfaces
      - visualisations
      - critical artworks

  - critical approach to neural networks
    - critical approach to data (CAPTA)
    - redescribing a neural networks as cultral computing
    - neural networks as cultural actors
    - training as enculturing
    - what are the semantics of internal representations

- originality
  - related work

</div>
